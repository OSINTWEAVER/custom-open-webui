services:
  open-webui-redis:
    container_name: open-webui-redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    volumes:
      - open-webui-valkey-data:/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  open-webui-searxng:
    container_name: open-webui-searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  open-webui-tika:
    container_name: open-webui-tika
    image: apache/tika:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:9998:9998"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  open-webui-osint-tools:
    container_name: open-webui-osint-tools
    build: ./openapi-tools
    restart: unless-stopped
    ports:
      - "127.0.0.1:8001:8001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  open-webui-mcp-proxy:
    container_name: open-webui-mcp-proxy
    build: ./mcp-servers
    restart: unless-stopped
    ports:
      - "127.0.0.1:8002:8002"
    environment:
      - MCP_SERVER_CMD=${MCP_SERVER_CMD:-uv run mcp-server-time --local-timezone=UTC}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  open-webui-litellm:
    container_name: open-webui-litellm
    image: ghcr.io/berriai/litellm:main-v1.59.0
    restart: unless-stopped
    ports:
      - "127.0.0.1:4010:4000"
    volumes:
      - ./open-webui-litellm-config:/app/config
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
    command: ["--config", "/app/config/config.yaml", "--port", "4000"]
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    restart: always
    ports:
      - "3000:8080"
    environment:
      # Privacy & Security Configuration
      ENABLE_COMMUNITY_SHARING: "false"
      ENABLE_MESSAGE_RATING: "false"
      ENABLE_EVALUATION_ARENA_MODELS: "false"
      WEBUI_SECRET_KEY: "${WEBUI_SECRET_KEY:-$(openssl rand -hex 32)}"
      
      # Disable Telemetry
      SCARF_NO_ANALYTICS: "true"
      DO_NOT_TRACK: "true"
      ANONYMIZED_TELEMETRY: "false"
      
      # RAG Web Search Configuration
      ENABLE_RAG_WEB_SEARCH: "true"
      RAG_WEB_SEARCH_ENGINE: "searxng"
      RAG_WEB_SEARCH_RESULT_COUNT: 10
      RAG_WEB_SEARCH_CONCURRENT_REQUESTS: 10
      SEARXNG_QUERY_URL: "http://open-webui-searxng:8080/search?q=<query>"
      RAG_WEB_SEARCH_DOMAIN_FILTER_LIST: ""
      
      # Document Processing with Tika
      ENABLE_RAG_LOCAL_WEB_FETCH: "true"
      TIKA_SERVER_URL: "http://open-webui-tika:9998"
      ENABLE_RAG_WEB_LOADER_SSL_VERIFICATION: "false"
      
      # Try to auto-configure document extraction
      CONTENT_EXTRACTION_ENGINE: "tika"
      DOC_EXTRACTION_ENGINE: "tika"
      
      # RAG Configuration
      ENABLE_RAG_HYBRID_SEARCH: "true"
      RAG_EMBEDDING_ENGINE: "ollama"
      RAG_EMBEDDING_MODEL: "snowflake-arctic-embed2:latest"
      RAG_RERANKING_MODEL: ""
      RAG_TOP_K: 5
      RAG_RELEVANCE_THRESHOLD: 0.0
      CHUNK_SIZE: 1600
      CHUNK_OVERLAP: 100
      
      # Enhanced File Support
      ENABLE_IMAGE_GENERATION: "true"
      ENABLE_LITELLM: "true"
      
      # Performance & Caching
      REDIS_URL: "redis://open-webui-redis:6379"
      ENABLE_MODEL_FILTER: "true"
      
      # Security Settings
      CORS_ALLOW_ORIGIN: "*"
      WEBUI_AUTH: "true"
      ENABLE_SIGNUP: "true"
      DEFAULT_USER_ROLE: "user"
      
      # Enhanced Integration URLs - External Ollama Support
      OLLAMA_BASE_URL: "${OLLAMA_BASE_URL:-http://192.168.2.241:11434}"
      LITELLM_BASE_URL: "http://open-webui-litellm:4000"
      
      # OpenAPI Tool Servers Configuration
      OPENAPI_SERVERS_CONFIG: |
        - name: "OSINT Tools"
          url: "http://open-webui-osint-tools:8001"
          description: "OSINT investigation tools including domain analysis, URL parsing, crypto utilities"
        - name: "MCP Proxy"
          url: "http://open-webui-mcp-proxy:8002"  
          description: "Model Context Protocol tools via OpenAPI proxy"
      
      # Default Model Configuration
      DEFAULT_MODELS: "gemma3:12b-it-q8_0"
      MODEL_FILTER_ENABLED: "true"
      
      # OSINT & Investigation Features
      ENABLE_RAG_WEB_SEARCH_DOMAIN_FILTER: "true"
      RAG_WEB_SEARCH_WHITELIST_DOMAINS: "archive.org,web.archive.org,duckduckgo.com,startpage.com,searx.space"
      
      # Logging & Monitoring
      LOG_LEVEL: "INFO"
      WEBUI_SESSION_COOKIE_SAME_SITE: "lax"
      
    volumes:
      - ./open-webui-data:/app/backend/data
    depends_on:
      - open-webui-redis
      - open-webui-searxng
      - open-webui-tika
      - open-webui-litellm
      - open-webui-osint-tools
      - open-webui-mcp-proxy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
volumes:
  open-webui-valkey-data:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=100m
  open-webui-litellm-config:
    driver: local
